{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98f661e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/navyachugh/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Cleaned file saved: metgala_Fauxmoi_2021_cleaned.json\n",
      " Cleaned file saved: metgala_Fauxmoi_2021_cleaned_cleaned.json\n",
      " Cleaned file saved: metgala_Fauxmoi_2024_cleaned.json\n",
      " Cleaned file saved: metgala_popculturechat_2024_cleaned.json\n",
      " Cleaned file saved: metgala_Fauxmoi_2025_cleaned.json\n",
      " Cleaned file saved: metgala_popculturechat_2025_cleaned.json\n",
      " Cleaned file saved: metgala_Fauxmoi_2022_cleaned.json\n",
      " Cleaned file saved: metgala_popculturechat_2022_cleaned.json\n",
      " Cleaned file saved: metgala_Fauxmoi_2023_cleaned.json\n",
      " Cleaned file saved: metgala_popculturechat_2023_cleaned.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Download NLTK stopwords if not already present\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# --- Preprocessing Function ---\n",
    "def preprocess_text(text, do_stem=True, remove_stopwords=True):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text)\n",
    "    text = re.sub(r'u\\/\\w+|@\\w+', '', text)\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    tokens = text.split()\n",
    "    if remove_stopwords:\n",
    "        tokens = [word for word in tokens if word not in stop_words]\n",
    "    if do_stem:\n",
    "        tokens = [stemmer.stem(word) for word in tokens]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# --- Process All JSON Files in Folder ---\n",
    "def preprocess_folder(folder_path):\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".json\"):\n",
    "            full_path = os.path.join(folder_path, filename)\n",
    "\n",
    "            with open(full_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                data = json.load(f)\n",
    "\n",
    "            # Preprocess each post and its comments\n",
    "            for post in data:\n",
    "                post[\"clean_title\"] = preprocess_text(post.get(\"title\", \"\"))\n",
    "                post[\"clean_selftext\"] = preprocess_text(post.get(\"selftext\", \"\"))\n",
    "                for comment in post.get(\"comments\", []):\n",
    "                    comment[\"clean_body\"] = preprocess_text(comment.get(\"body\", \"\"))\n",
    "\n",
    "            # Save to new file\n",
    "            new_filename = filename.replace(\".json\", \"_cleaned.json\")\n",
    "            new_path = os.path.join(folder_path, new_filename)\n",
    "            with open(new_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "            print(f\" Cleaned file saved: {new_filename}\")\n",
    "\n",
    "# --- Run it ---\n",
    "folder_path = \"metgala_data\" \n",
    "preprocess_folder(folder_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "199f1b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load your CSV of attendees\n",
    "celeb_df = pd.read_csv(\"attendees.csv\")\n",
    "celeb_names = celeb_df['Name'].dropna().unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dcacf27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Normalize names\n",
    "celeb_patterns = [re.escape(name.lower()) for name in celeb_names]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90e5c3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_mentioned_celebs(text, celeb_list):\n",
    "    text = text.lower()\n",
    "    mentioned = []\n",
    "    for celeb in celeb_list:\n",
    "        if celeb in text:\n",
    "            mentioned.append(celeb)\n",
    "    return mentioned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af3e6856",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load your Reddit data (adjust filename as needed)\n",
    "with open(\"metgala_data/metgala_popculturechat_2025.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    reddit_data = json.load(f)\n",
    "\n",
    "# Scan all posts + comments for celeb mentions\n",
    "mention_summary = {}\n",
    "\n",
    "for post in reddit_data:\n",
    "    post_text = post['title'] + \" \" + post.get('selftext', '')\n",
    "    comments = post.get(\"comments\", [])\n",
    "\n",
    "    # Check post itself\n",
    "    mentioned = find_mentioned_celebs(post_text, celeb_patterns)\n",
    "    for name in mentioned:\n",
    "        mention_summary[name] = mention_summary.get(name, 0) + 1\n",
    "\n",
    "    # Check comments\n",
    "    for comment in comments:\n",
    "        comment_text = comment.get('body', '')\n",
    "        mentioned = find_mentioned_celebs(comment_text, celeb_patterns)\n",
    "        for name in mentioned:\n",
    "            mention_summary[name] = mention_summary.get(name, 0) + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "771e7798",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zendaya: 192 mentions\n",
      "doechii: 88 mentions\n",
      "rihanna: 61 mentions\n",
      "usher: 33 mentions\n",
      "future: 20 mentions\n",
      "shakira: 15 mentions\n",
      "lorde: 15 mentions\n",
      "madonna: 12 mentions\n",
      "ciara: 10 mentions\n",
      "maluma: 10 mentions\n",
      "babyface: 5 mentions\n",
      "lizzo: 3 mentions\n",
      "rosÃ©: 2 mentions\n",
      "sza: 2 mentions\n",
      "iman: 2 mentions\n",
      "finneas: 1 mentions\n"
     ]
    }
   ],
   "source": [
    "# Sort by frequency\n",
    "sorted_mentions = sorted(mention_summary.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "for celeb, count in sorted_mentions:  # top 20\n",
    "    print(f\"{celeb}: {count} mentions\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "51e4b5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "abbreviations = {\n",
    "    \"$\" : \" dollar \",\n",
    "    \"â‚¬\" : \" euro \",\n",
    "    \"4ao\" : \"for adults only\",\n",
    "    \"a.m\" : \"before midday\",\n",
    "    \"a3\" : \"anytime anywhere anyplace\",\n",
    "    \"aamof\" : \"as a matter of fact\",\n",
    "    \"acct\" : \"account\",\n",
    "    \"adih\" : \"another day in hell\",\n",
    "    \"afaic\" : \"as far as i am concerned\",\n",
    "    \"afaict\" : \"as far as i can tell\",\n",
    "    \"afaik\" : \"as far as i know\",\n",
    "    \"afair\" : \"as far as i remember\",\n",
    "    \"afk\" : \"away from keyboard\",\n",
    "    \"app\" : \"application\",\n",
    "    \"approx\" : \"approximately\",\n",
    "    \"apps\" : \"applications\",\n",
    "    \"asap\" : \"as soon as possible\",\n",
    "    \"asl\" : \"age, sex, location\",\n",
    "    \"atk\" : \"at the keyboard\",\n",
    "    \"ave.\" : \"avenue\",\n",
    "    \"aymm\" : \"are you my mother\",\n",
    "    \"ayor\" : \"at your own risk\", \n",
    "    \"b&b\" : \"bed and breakfast\",\n",
    "    \"b+b\" : \"bed and breakfast\",\n",
    "    \"b.c\" : \"before christ\",\n",
    "    \"b2b\" : \"business to business\",\n",
    "    \"b2c\" : \"business to customer\",\n",
    "    \"b4\" : \"before\",\n",
    "    \"b4n\" : \"bye for now\",\n",
    "    \"b@u\" : \"back at you\",\n",
    "    \"bae\" : \"before anyone else\",\n",
    "    \"bak\" : \"back at keyboard\",\n",
    "    \"bbbg\" : \"bye bye be good\",\n",
    "    \"bbc\" : \"british broadcasting corporation\",\n",
    "    \"bbias\" : \"be back in a second\",\n",
    "    \"bbl\" : \"be back later\",\n",
    "    \"bbs\" : \"be back soon\",\n",
    "    \"be4\" : \"before\",\n",
    "    \"bfn\" : \"bye for now\",\n",
    "    \"blvd\" : \"boulevard\",\n",
    "    \"bout\" : \"about\",\n",
    "    \"brb\" : \"be right back\",\n",
    "    \"bros\" : \"brothers\",\n",
    "    \"brt\" : \"be right there\",\n",
    "    \"bsaaw\" : \"big smile and a wink\",\n",
    "    \"btw\" : \"by the way\",\n",
    "    \"bwl\" : \"bursting with laughter\",\n",
    "    \"c/o\" : \"care of\",\n",
    "    \"cet\" : \"central european time\",\n",
    "    \"cf\" : \"compare\",\n",
    "    \"cia\" : \"central intelligence agency\",\n",
    "    \"csl\" : \"can not stop laughing\",\n",
    "    \"cu\" : \"see you\",\n",
    "    \"cul8r\" : \"see you later\",\n",
    "    \"cv\" : \"curriculum vitae\",\n",
    "    \"cwot\" : \"complete waste of time\",\n",
    "    \"cya\" : \"see you\",\n",
    "    \"cyt\" : \"see you tomorrow\",\n",
    "    \"dae\" : \"does anyone else\",\n",
    "    \"dbmib\" : \"do not bother me i am busy\",\n",
    "    \"diy\" : \"do it yourself\",\n",
    "    \"dm\" : \"direct message\",\n",
    "    \"dwh\" : \"during work hours\",\n",
    "    \"e123\" : \"easy as one two three\",\n",
    "    \"eet\" : \"eastern european time\",\n",
    "    \"eg\" : \"example\",\n",
    "    \"embm\" : \"early morning business meeting\",\n",
    "    \"encl\" : \"enclosed\",\n",
    "    \"encl.\" : \"enclosed\",\n",
    "    \"etc\" : \"and so on\",\n",
    "    \"faq\" : \"frequently asked questions\",\n",
    "    \"fawc\" : \"for anyone who cares\",\n",
    "    \"fb\" : \"facebook\",\n",
    "    \"fc\" : \"fingers crossed\",\n",
    "    \"fig\" : \"figure\",\n",
    "    \"fimh\" : \"forever in my heart\", \n",
    "    \"ft.\" : \"feet\",\n",
    "    \"ft\" : \"featuring\",\n",
    "    \"ftl\" : \"for the loss\",\n",
    "    \"ftw\" : \"for the win\",\n",
    "    \"fwiw\" : \"for what it is worth\",\n",
    "    \"fyi\" : \"for your information\",\n",
    "    \"g9\" : \"genius\",\n",
    "    \"gahoy\" : \"get a hold of yourself\",\n",
    "    \"gal\" : \"get a life\",\n",
    "    \"gcse\" : \"general certificate of secondary education\",\n",
    "    \"gfn\" : \"gone for now\",\n",
    "    \"gg\" : \"good game\",\n",
    "    \"gl\" : \"good luck\",\n",
    "    \"glhf\" : \"good luck have fun\",\n",
    "    \"gmt\" : \"greenwich mean time\",\n",
    "    \"gmta\" : \"great minds think alike\",\n",
    "    \"gn\" : \"good night\",\n",
    "    \"g.o.a.t\" : \"greatest of all time\",\n",
    "    \"goat\" : \"greatest of all time\",\n",
    "    \"goi\" : \"get over it\",\n",
    "    \"gps\" : \"global positioning system\",\n",
    "    \"gr8\" : \"great\",\n",
    "    \"gratz\" : \"congratulations\",\n",
    "    \"gyal\" : \"girl\",\n",
    "    \"h&c\" : \"hot and cold\",\n",
    "    \"hp\" : \"horsepower\",\n",
    "    \"hr\" : \"hour\",\n",
    "    \"hrh\" : \"his royal highness\",\n",
    "    \"ht\" : \"height\",\n",
    "    \"ibrb\" : \"i will be right back\",\n",
    "    \"ic\" : \"i see\",\n",
    "    \"icq\" : \"i seek you\",\n",
    "    \"icymi\" : \"in case you missed it\",\n",
    "    \"idc\" : \"i do not care\",\n",
    "    \"idgadf\" : \"i do not give a damn fuck\",\n",
    "    \"idgaf\" : \"i do not give a fuck\",\n",
    "    \"idk\" : \"i do not know\",\n",
    "    \"ie\" : \"that is\",\n",
    "    \"i.e\" : \"that is\",\n",
    "    \"ifyp\" : \"i feel your pain\",\n",
    "    \"IG\" : \"instagram\",\n",
    "    \"iirc\" : \"if i remember correctly\",\n",
    "    \"ilu\" : \"i love you\",\n",
    "    \"ily\" : \"i love you\",\n",
    "    \"imho\" : \"in my humble opinion\",\n",
    "    \"imo\" : \"in my opinion\",\n",
    "    \"imu\" : \"i miss you\",\n",
    "    \"iow\" : \"in other words\",\n",
    "    \"irl\" : \"in real life\",\n",
    "    \"j4f\" : \"just for fun\",\n",
    "    \"jic\" : \"just in case\",\n",
    "    \"jk\" : \"just kidding\",\n",
    "    \"jsyk\" : \"just so you know\",\n",
    "    \"l8r\" : \"later\",\n",
    "    \"lb\" : \"pound\",\n",
    "    \"lbs\" : \"pounds\",\n",
    "    \"ldr\" : \"long distance relationship\",\n",
    "    \"lmao\" : \"laugh my ass off\",\n",
    "    \"lmfao\" : \"laugh my fucking ass off\",\n",
    "    \"lol\" : \"laughing out loud\",\n",
    "    \"ltd\" : \"limited\",\n",
    "    \"ltns\" : \"long time no see\",\n",
    "    \"m8\" : \"mate\",\n",
    "    \"mf\" : \"motherfucker\",\n",
    "    \"mfs\" : \"motherfuckers\",\n",
    "    \"mfw\" : \"my face when\",\n",
    "    \"mofo\" : \"motherfucker\",\n",
    "    \"mph\" : \"miles per hour\",\n",
    "    \"mr\" : \"mister\",\n",
    "    \"mrw\" : \"my reaction when\",\n",
    "    \"ms\" : \"miss\",\n",
    "    \"mte\" : \"my thoughts exactly\",\n",
    "    \"nagi\" : \"not a good idea\",\n",
    "    \"nbc\" : \"national broadcasting company\",\n",
    "    \"nbd\" : \"not big deal\",\n",
    "    \"nfs\" : \"not for sale\",\n",
    "    \"ngl\" : \"not going to lie\",\n",
    "    \"nhs\" : \"national health service\",\n",
    "    \"nrn\" : \"no reply necessary\",\n",
    "    \"nsfl\" : \"not safe for life\",\n",
    "    \"nsfw\" : \"not safe for work\",\n",
    "    \"nth\" : \"nice to have\",\n",
    "    \"nvr\" : \"never\",\n",
    "    \"nyc\" : \"new york city\",\n",
    "    \"oc\" : \"original content\",\n",
    "    \"og\" : \"original\",\n",
    "    \"ohp\" : \"overhead projector\",\n",
    "    \"oic\" : \"oh i see\",\n",
    "    \"omdb\" : \"over my dead body\",\n",
    "    \"omg\" : \"oh my god\",\n",
    "    \"omw\" : \"on my way\",\n",
    "    \"p.a\" : \"per annum\",\n",
    "    \"p.m\" : \"after midday\",\n",
    "    \"pm\" : \"prime minister\",\n",
    "    \"poc\" : \"people of color\",\n",
    "    \"pov\" : \"point of view\",\n",
    "    \"pp\" : \"pages\",\n",
    "    \"ppl\" : \"people\",\n",
    "    \"prw\" : \"parents are watching\",\n",
    "    \"ps\" : \"postscript\",\n",
    "    \"pt\" : \"point\",\n",
    "    \"ptb\" : \"please text back\",\n",
    "    \"pto\" : \"please turn over\",\n",
    "    \"qpsa\" : \"what happens\", #\"que pasa\",\n",
    "    \"ratchet\" : \"rude\",\n",
    "    \"rbtl\" : \"read between the lines\",\n",
    "    \"rlrt\" : \"real life retweet\", \n",
    "    \"rofl\" : \"rolling on the floor laughing\",\n",
    "    \"roflol\" : \"rolling on the floor laughing out loud\",\n",
    "    \"rotflmao\" : \"rolling on the floor laughing my ass off\",\n",
    "    \"rt\" : \"retweet\",\n",
    "    \"ruok\" : \"are you ok\",\n",
    "    \"sfw\" : \"safe for work\",\n",
    "    \"sk8\" : \"skate\",\n",
    "    \"smh\" : \"shake my head\",\n",
    "    \"sq\" : \"square\",\n",
    "    \"srsly\" : \"seriously\", \n",
    "    \"ssdd\" : \"same stuff different day\",\n",
    "    \"tbh\" : \"to be honest\",\n",
    "    \"tbs\" : \"tablespooful\",\n",
    "    \"tbsp\" : \"tablespooful\",\n",
    "    \"tfw\" : \"that feeling when\",\n",
    "    \"thks\" : \"thank you\",\n",
    "    \"tho\" : \"though\",\n",
    "    \"thx\" : \"thank you\",\n",
    "    \"tia\" : \"thanks in advance\",\n",
    "    \"til\" : \"today i learned\",\n",
    "    \"tl;dr\" : \"too long i did not read\",\n",
    "    \"tldr\" : \"too long i did not read\",\n",
    "    \"tmb\" : \"tweet me back\",\n",
    "    \"tntl\" : \"trying not to laugh\",\n",
    "    \"ttyl\" : \"talk to you later\",\n",
    "    \"u\" : \"you\",\n",
    "    \"u2\" : \"you too\",\n",
    "    \"u4e\" : \"yours for ever\",\n",
    "    \"utc\" : \"coordinated universal time\",\n",
    "    \"w/\" : \"with\",\n",
    "    \"w/o\" : \"without\",\n",
    "    \"w8\" : \"wait\",\n",
    "    \"wassup\" : \"what is up\",\n",
    "    \"wb\" : \"welcome back\",\n",
    "    \"wtf\" : \"what the fuck\",\n",
    "    \"wtg\" : \"way to go\",\n",
    "    \"wtpa\" : \"where the party at\",\n",
    "    \"wuf\" : \"where are you from\",\n",
    "    \"wuzup\" : \"what is up\",\n",
    "    \"wywh\" : \"wish you were here\",\n",
    "    \"yd\" : \"yard\",\n",
    "    \"ygtr\" : \"you got that right\",\n",
    "    \"ynk\" : \"you never know\",\n",
    "    \"zzz\" : \"sleeping bored and tired\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d6ac36c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text, abbreviations):\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Replace abbreviations/slang\n",
    "    for abbr, full_form in abbreviations.items():\n",
    "        pattern = r'\\b' + re.escape(abbr) + r'\\b'\n",
    "        text = re.sub(pattern, full_form, text)\n",
    "\n",
    "    # Remove URLs\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text, flags=re.MULTILINE)\n",
    "\n",
    "    # Remove non-alphanumeric characters (except spaces)\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "\n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f41939",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aebfb9bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/navyachugh/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/navyachugh/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/navyachugh/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "# Setup\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Emoji pattern\n",
    "emoji_pattern = re.compile(\n",
    "    \"[\"\n",
    "    u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "    u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "    u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "    u\"\\U0001F1E0-\\U0001F1FF\"  # flags\n",
    "    u\"\\U00002700-\\U000027BF\"\n",
    "    u\"\\U000024C2-\\U0001F251\"\n",
    "    \"]+\", flags=re.UNICODE\n",
    ")\n",
    "\n",
    "def preprocess_text(text, slang_dict, use_stemming=False, use_lemmatization=True):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r\"http\\S+|www.\\S+\", \"\", text)\n",
    "    \n",
    "    # Remove emojis\n",
    "    text = emoji_pattern.sub(r'', text)\n",
    "    \n",
    "    # Replace abbreviations/slang\n",
    "    words = text.split()\n",
    "    words = [slang_dict.get(word, word) for word in words]\n",
    "\n",
    "    # Remove punctuation\n",
    "    words = [word.translate(str.maketrans('', '', string.punctuation)) for word in words]\n",
    "    \n",
    "    # Remove stopwords\n",
    "    words = [word for word in words if word and word not in stop_words]\n",
    "\n",
    "    # Stemming or Lemmatization\n",
    "    if use_stemming:\n",
    "        words = [stemmer.stem(word) for word in words]\n",
    "    elif use_lemmatization:\n",
    "        words = [lemmatizer.lemmatize(word) for word in words]\n",
    "\n",
    "    # Join words\n",
    "    return \" \".join(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "68512739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Preprocessing metgala_Fauxmoi_2021_cleaned_cleaned.json...\n",
      " Preprocessing metgala_Fauxmoi_2021.json...\n",
      " Preprocessing metgala_Fauxmoi_2025_cleaned.json...\n",
      " Preprocessing metgala_Fauxmoi_2022_cleaned.json...\n",
      " Preprocessing metgala_Fauxmoi_2023_cleaned.json...\n",
      " Preprocessing metgala_Fauxmoi_2024_cleaned.json...\n",
      " Preprocessing metgala_Fauxmoi_2021_cleaned.json...\n",
      " Preprocessing metgala_Fauxmoi_2024.json...\n",
      " Preprocessing metgala_popculturechat_2024.json...\n",
      " Preprocessing metgala_Fauxmoi_2025.json...\n",
      " Preprocessing metgala_popculturechat_2025.json...\n",
      " Preprocessing metgala_Fauxmoi_2022.json...\n",
      " Preprocessing metgala_popculturechat_2022.json...\n",
      " Preprocessing metgala_popculturechat_2023_cleaned.json...\n",
      " Preprocessing metgala_popculturechat_2024_cleaned.json...\n",
      " Preprocessing metgala_Fauxmoi_2023.json...\n",
      " Preprocessing metgala_popculturechat_2023.json...\n",
      " Preprocessing metgala_popculturechat_2025_cleaned.json...\n",
      " Preprocessing metgala_popculturechat_2022_cleaned.json...\n",
      " All files preprocessed!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# Define your folder paths\n",
    "input_folder = \"metgala_data\"\n",
    "output_folder = \"metgala_data_cleaned\"\n",
    "\n",
    "# Make sure output folder exists\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Loop through all JSON files in the folder\n",
    "for filename in os.listdir(input_folder):\n",
    "    if filename.endswith(\".json\"):\n",
    "        print(f\" Preprocessing {filename}...\")\n",
    "\n",
    "        input_path = os.path.join(input_folder, filename)\n",
    "        output_path = os.path.join(output_folder, filename)\n",
    "\n",
    "        # Load file\n",
    "        with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        # Preprocess each post and comment\n",
    "        for post in data:\n",
    "            post['title'] = preprocess_text(post.get('title', ''), abbreviations)\n",
    "            post['selftext'] = preprocess_text(post.get('selftext', ''), abbreviations)\n",
    "            \n",
    "            for comment in post.get('comments', []):\n",
    "                comment['body'] = preprocess_text(comment.get('body', ''), abbreviations)\n",
    "\n",
    "        # Save preprocessed data\n",
    "        with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\" All files preprocessed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "60121260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Celebrity Mentions:\n",
      "zendaya: 172 mentions\n",
      "rihanna: 36 mentions\n",
      "doechii: 34 mentions\n",
      "diana ross: 33 mentions\n",
      "anna wintour: 27 mentions\n",
      "walton goggins: 25 mentions\n",
      "usher: 23 mentions\n",
      "colman domingo: 21 mentions\n",
      "lewis hamilton: 18 mentions\n",
      "thom browne: 16 mentions\n",
      "zac posen: 16 mentions\n",
      "halle berry: 15 mentions\n",
      "janelle monae: 15 mentions\n",
      "sabrina carpenter: 15 mentions\n",
      "cardi b: 12 mentions\n",
      "anna sawai: 11 mentions\n",
      "teyana taylor: 11 mentions\n",
      "doja cat: 10 mentions\n",
      "emma chamberlain: 10 mentions\n",
      "ciara: 10 mentions\n",
      "lauryn hill: 10 mentions\n",
      "megan thee stallion: 9 mentions\n",
      "future: 9 mentions\n",
      "madonna: 9 mentions\n",
      "lorde: 9 mentions\n",
      "alton mason: 9 mentions\n",
      "anne hathaway: 9 mentions\n",
      "laura harrier: 8 mentions\n",
      "sarah snook: 8 mentions\n",
      "bad bunny: 7 mentions\n",
      "james corden: 7 mentions\n",
      "sydney sweeney: 7 mentions\n",
      "tessa thompson: 7 mentions\n",
      "coco jones: 7 mentions\n",
      "ayo edebiri: 6 mentions\n",
      "hunter schafer: 6 mentions\n",
      "karlie kloss: 6 mentions\n",
      "andrew scott: 6 mentions\n",
      "maya hawke: 6 mentions\n",
      "janelle monÃ¡e: 6 mentions\n",
      "monica barbaro: 6 mentions\n",
      "angela bassett: 6 mentions\n",
      "debbie allen: 6 mentions\n",
      "frank ocean: 6 mentions\n",
      "pamela anderson: 5 mentions\n",
      "gabrielle union: 5 mentions\n",
      "demi moore: 5 mentions\n",
      "chappell roan: 5 mentions\n",
      "gigi hadid: 5 mentions\n",
      "hailey bieber: 5 mentions\n",
      "kim kardashian: 5 mentions\n",
      "pharrell williams: 5 mentions\n",
      "damson idris: 5 mentions\n",
      "bradley cooper: 4 mentions\n",
      "kendall jenner: 4 mentions\n",
      "serena williams: 4 mentions\n",
      "nicole kidman: 4 mentions\n",
      "cynthia erivo: 4 mentions\n",
      "troye sivan: 4 mentions\n",
      "jon batiste: 4 mentions\n",
      "dapper dan: 4 mentions\n",
      "brian tyree henry: 4 mentions\n",
      "billie eilish: 3 mentions\n",
      "natasha lyonne: 3 mentions\n",
      "sadie sink: 3 mentions\n",
      "quinta brunson: 3 mentions\n",
      "dua lipa: 3 mentions\n",
      "charli xcx: 3 mentions\n",
      "jared leto: 3 mentions\n",
      "jenna ortega: 3 mentions\n",
      "nicki minaj: 3 mentions\n",
      "jeremy pope: 3 mentions\n",
      "suki waterhouse: 3 mentions\n",
      "venus williams: 3 mentions\n",
      "anok yai: 3 mentions\n",
      "imaan hammam: 3 mentions\n",
      "andrÃ© 3000: 3 mentions\n",
      "miley cyrus: 3 mentions\n",
      "lizzo: 3 mentions\n",
      "maluma: 3 mentions\n",
      "mindy kaling: 3 mentions\n",
      "jasmine tookes: 2 mentions\n",
      "shakira: 2 mentions\n",
      "vittoria ceretti: 2 mentions\n",
      "kylie jenner: 2 mentions\n",
      "nicola coughlan: 2 mentions\n",
      "saquon barkley: 2 mentions\n",
      "dwyane wade: 2 mentions\n",
      "aimee lou wood: 2 mentions\n",
      "jaden smith: 2 mentions\n",
      "patrick schwarzenegger: 2 mentions\n",
      "helen lasichanh: 2 mentions\n",
      "tramell tillman: 2 mentions\n",
      "tory burch: 2 mentions\n",
      "michelle yeoh: 2 mentions\n",
      "ego nwodim: 2 mentions\n",
      "ugbad abdi: 2 mentions\n",
      "christian latchman: 2 mentions\n",
      "jordan roth: 2 mentions\n",
      "jeremy allen white: 2 mentions\n",
      "zuri hall: 2 mentions\n",
      "whoopi goldberg: 2 mentions\n",
      "evan ross: 1 mentions\n",
      "barry keoghan: 1 mentions\n",
      "pedro pascal: 1 mentions\n",
      "lana del rey: 1 mentions\n",
      "sofia richie: 1 mentions\n",
      "rachel brosnahan: 1 mentions\n",
      "regina king: 1 mentions\n",
      "jordan casteel: 1 mentions\n",
      "radhika jones: 1 mentions\n",
      "blake lively: 1 mentions\n",
      "ariana grande: 1 mentions\n",
      "cara delevingne: 1 mentions\n",
      "justin bieber: 1 mentions\n",
      "timothÃ©e chalamet: 1 mentions\n",
      "emma stone: 1 mentions\n",
      "saweetie: 1 mentions\n",
      "shawn mendes: 1 mentions\n",
      "erykah badu: 1 mentions\n",
      "tom ford: 1 mentions\n",
      "haider ackermann: 1 mentions\n",
      "bella hadid: 1 mentions\n",
      "hillary clinton: 1 mentions\n",
      "sharon stone: 1 mentions\n",
      "gwen stefani: 1 mentions\n",
      "amy schumer: 1 mentions\n",
      "raÃºl domingo: 1 mentions\n",
      "kerry washington: 1 mentions\n",
      "andra day: 1 mentions\n",
      "halle bailey: 1 mentions\n",
      "adut akech: 1 mentions\n",
      "swizz beatz: 1 mentions\n",
      "audra mcdonald: 1 mentions\n",
      "keke palmer: 1 mentions\n",
      "yara shahidi: 1 mentions\n",
      "colin kaepernick: 1 mentions\n",
      "sebastian stan: 1 mentions\n",
      "karol g: 1 mentions\n",
      "kaia gerber: 1 mentions\n",
      "janicza bravo: 1 mentions\n",
      "uma thurman: 1 mentions\n",
      "megan fox: 1 mentions\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Load attendees list (make sure it's cleaned & lowercase)\n",
    "celeb_df = pd.read_csv(\"attendees.csv\")\n",
    "celeb_names = celeb_df['Name'].dropna().unique()\n",
    "celeb_names = [name.lower() for name in celeb_names]\n",
    "\n",
    "# Optional: escape special characters in names\n",
    "escaped_names = [re.escape(name) for name in celeb_names]\n",
    "\n",
    "# Create a pattern to match any celeb name as a word\n",
    "celeb_pattern = re.compile(r'\\b(' + '|'.join(escaped_names) + r')\\b', re.IGNORECASE)\n",
    "\n",
    "# Load the preprocessed Reddit file\n",
    "with open(\"metgala_data_cleaned/metgala_Fauxmoi_2025.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    reddit_data = json.load(f)\n",
    "\n",
    "# Track mentions\n",
    "mention_counts = {}\n",
    "\n",
    "# Check mentions in post + comments\n",
    "for post in reddit_data:\n",
    "    text = f\"{post.get('title', '')} {post.get('selftext', '')}\"\n",
    "    for match in celeb_pattern.findall(text):\n",
    "        match = match.lower()\n",
    "        mention_counts[match] = mention_counts.get(match, 0) + 1\n",
    "\n",
    "    for comment in post.get(\"comments\", []):\n",
    "        text = comment.get(\"body\", \"\")\n",
    "        for match in celeb_pattern.findall(text):\n",
    "            match = match.lower()\n",
    "            mention_counts[match] = mention_counts.get(match, 0) + 1\n",
    "\n",
    "# Sort and display results\n",
    "sorted_mentions = sorted(mention_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\" Celebrity Mentions:\")\n",
    "for name, count in sorted_mentions:\n",
    "    print(f\"{name}: {count} mentions\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1f0b889a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Nodes (users): 9688\n",
      "Total Edges (interactions): 1039230\n",
      "\n",
      " Top 10 Central Users:\n",
      "AutoModerator: 0.4371\n",
      "Cultural-Party1876: 0.2486\n",
      "Rude_Lifeguard: 0.2347\n",
      "Educational-Help-126: 0.2323\n",
      "InitiativeSad1021: 0.2205\n",
      "Luna_Soma: 0.2183\n",
      "Ester_LoverGirl: 0.2051\n",
      "trulyremarkablegirl: 0.2028\n",
      "lavabread23: 0.1953\n",
      "PsychologicalClue6: 0.1900\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import networkx as nx\n",
    "import os\n",
    "\n",
    "# --- Step 1: Load JSON files ---\n",
    "def load_reddit_json(path):\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "# Update these paths to your actual file locations\n",
    "data1 = load_reddit_json('metgala_data_cleaned/metgala_popculturechat_2025.json')\n",
    "data2 = load_reddit_json('metgala_data_cleaned/metgala_Fauxmoi_2025.json')\n",
    "\n",
    "# --- Step 2: Combine datasets ---\n",
    "combined_data = data1 + data2\n",
    "\n",
    "# --- Step 3: Build User Interaction Graph ---\n",
    "G = nx.Graph()\n",
    "\n",
    "for post in combined_data:\n",
    "    post_author = post.get(\"author\", \"N/A\")\n",
    "    commenters = [comment[\"author\"] for comment in post.get(\"comments\", []) if comment[\"author\"] != \"N/A\"]\n",
    "\n",
    "    # Add edges between post author and each commenter\n",
    "    for commenter in commenters:\n",
    "        if post_author != \"N/A\":\n",
    "            G.add_edge(post_author, commenter)\n",
    "    \n",
    "    # Optionally: connect commenters to each other if they reply under same post\n",
    "    for i in range(len(commenters)):\n",
    "        for j in range(i + 1, len(commenters)):\n",
    "            G.add_edge(commenters[i], commenters[j])\n",
    "\n",
    "# --- Step 4: Analyze Graph ---\n",
    "print(f\"Total Nodes (users): {G.number_of_nodes()}\")\n",
    "print(f\"Total Edges (interactions): {G.number_of_edges()}\")\n",
    "\n",
    "# Degree centrality (most connected users)\n",
    "centrality = nx.degree_centrality(G)\n",
    "top_users = sorted(centrality.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "\n",
    "print(\"\\n Top 10 Central Users:\")\n",
    "for user, score in top_users:\n",
    "    print(f\"{user}: {score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bd9be0ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Total Nodes: 1298\n",
      " Total Edges: 1648\n",
      "\n",
      " Top Mentioned Celebrities:\n",
      "Zendaya: 267 mentions\n",
      "Doechii: 90 mentions\n",
      "Rihanna: 71 mentions\n",
      "Diana Ross: 56 mentions\n",
      "Usher: 45 mentions\n",
      "Anna Wintour: 41 mentions\n",
      "Thom Browne: 39 mentions\n",
      "Cardi B: 35 mentions\n",
      "Lewis Hamilton: 35 mentions\n",
      "Sabrina Carpenter: 30 mentions\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Step 1: Load Celebs\n",
    "celeb_df = pd.read_csv(\"attendees.csv\")\n",
    "celeb_names = celeb_df['Name'].dropna().unique()\n",
    "celeb_names_normalized = [name.lower() for name in celeb_names]\n",
    "\n",
    "# Optional: compile regex pattern for speed\n",
    "celeb_patterns = [re.escape(name.lower()) for name in celeb_names_normalized]\n",
    "celeb_pattern = re.compile(r'\\b(?:' + '|'.join(celeb_patterns) + r')\\b')\n",
    "\n",
    "# Step 2: Build Graph\n",
    "G = nx.Graph()\n",
    "\n",
    "for post in combined_data:\n",
    "    comments = post.get(\"comments\", [])\n",
    "    \n",
    "    for comment in comments:\n",
    "        author = comment.get(\"author\", \"N/A\")\n",
    "        if author == \"N/A\":\n",
    "            continue\n",
    "        \n",
    "        body = comment.get(\"body\", \"\").lower()\n",
    "        mentioned_celebs = celeb_pattern.findall(body)\n",
    "\n",
    "        for celeb in mentioned_celebs:\n",
    "            celeb = celeb.strip().lower()\n",
    "            G.add_node(author, type=\"user\")\n",
    "            G.add_node(celeb, type=\"celeb\")\n",
    "            G.add_edge(author, celeb)\n",
    "\n",
    "# Step 3: Print Summary\n",
    "print(f\" Total Nodes: {G.number_of_nodes()}\")\n",
    "print(f\" Total Edges: {G.number_of_edges()}\")\n",
    "\n",
    "# Optional: Most mentioned celebrities\n",
    "celeb_mentions = {}\n",
    "for node in G.nodes(data=True):\n",
    "    if node[1].get(\"type\") == \"celeb\":\n",
    "        celeb_mentions[node[0]] = G.degree(node[0])\n",
    "\n",
    "sorted_mentions = sorted(celeb_mentions.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "print(\"\\n Top Mentioned Celebrities:\")\n",
    "for name, count in sorted_mentions:\n",
    "    print(f\"{name.title()}: {count} mentions\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8e45cff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " User interaction graph saved as 'user_interaction_graph_2025.gexf'\n"
     ]
    }
   ],
   "source": [
    "# Save user interaction graph\n",
    "nx.write_gexf(G, \"user_interaction_graph_2025.gexf\")\n",
    "print(\" User interaction graph saved as 'user_interaction_graph_2025.gexf'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "627ee582",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Celebrity mention graph saved as 'celebrity_mention_graph_2025.gexf'\n"
     ]
    }
   ],
   "source": [
    "# Save celebrity mention graph\n",
    "nx.write_gexf(G, \"celebrity_mention_graph_2025.gexf\")\n",
    "print(\"Celebrity mention graph saved as 'celebrity_mention_graph_2025.gexf'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "85f808cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for post in combined_data:\n",
    "    subreddit = post.get(\"subreddit\", \"unknown\")  # Add this if your data has subreddit info\n",
    "    post_author = post.get(\"author\", \"N/A\")\n",
    "    \n",
    "    if post_author != \"N/A\":\n",
    "        G.add_node(post_author, subreddit=subreddit)\n",
    "\n",
    "    for comment in post.get(\"comments\", []):\n",
    "        author = comment.get(\"author\", \"N/A\")\n",
    "        if author != \"N/A\":\n",
    "            G.add_node(author, subreddit=subreddit)\n",
    "            G.add_edge(post_author, author)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18e102f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total nodes: 5188\n",
      "Total edges: 550239\n",
      "Graph exported as 'community_detection_graph.graphml' for Gephi.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import community as community_louvain  # python-louvain\n",
    "\n",
    "# -------- Step 1: Load and label data --------\n",
    "def load_data(path, subreddit_name):\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    for post in data:\n",
    "        post['subreddit'] = subreddit_name\n",
    "    return data\n",
    "\n",
    "data_fauxmoi = load_data('metgala_data_cleaned/metgala_Fauxmoi_2023.json', 'Fauxmoi')\n",
    "data_popculture = load_data('metgala_data_cleaned/metgala_popculturechat_2023.json', 'PopCultureChat')\n",
    "combined_data = data_fauxmoi + data_popculture\n",
    "\n",
    "# -------- Step 2: Build User Interaction Graph --------\n",
    "G = nx.Graph()\n",
    "\n",
    "for post in combined_data:\n",
    "    subreddit = post.get(\"subreddit\", \"Unknown\")\n",
    "    post_author = post.get(\"author\", \"N/A\")\n",
    "    commenters = [c.get(\"author\", \"N/A\") for c in post.get(\"comments\", []) if c.get(\"author\", \"N/A\") != \"N/A\"]\n",
    "\n",
    "    # Add post author\n",
    "    if post_author != \"N/A\":\n",
    "        G.add_node(post_author, subreddit=subreddit)\n",
    "\n",
    "    # Add commenters and edges\n",
    "    for commenter in commenters:\n",
    "        G.add_node(commenter, subreddit=subreddit)\n",
    "        if post_author != \"N/A\":\n",
    "            G.add_edge(post_author, commenter)\n",
    "    \n",
    "    # Optionally connect commenters to each other\n",
    "    for i in range(len(commenters)):\n",
    "        for j in range(i + 1, len(commenters)):\n",
    "            G.add_edge(commenters[i], commenters[j])\n",
    "\n",
    "print(f\"Total nodes: {G.number_of_nodes()}\")\n",
    "print(f\"Total edges: {G.number_of_edges()}\")\n",
    "\n",
    "# -------- Step 3: Run Louvain Community Detection --------\n",
    "partition = community_louvain.best_partition(G)\n",
    "nx.set_node_attributes(G, partition, 'community')\n",
    "\n",
    "# -------- Step 4: Export for Gephi --------\n",
    "# Export as GraphML (supports node attributes like 'subreddit' and 'community')\n",
    "nx.write_graphml(G, \"community_detection_graph.graphml\")\n",
    "print(\"Graph exported as 'community_detection_graph.graphml' for Gephi.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81c300d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered graph: 5176 nodes, 550178 edges\n"
     ]
    }
   ],
   "source": [
    "# Filter to active users\n",
    "active_users = [n for n, d in G.degree() if d >= 10]\n",
    "G_active = G.subgraph(active_users).copy()\n",
    "\n",
    "print(f\"Filtered graph: {G_active.number_of_nodes()} nodes, {G_active.number_of_edges()} edges\")\n",
    "nx.write_graphml(G_active, \"community_detection_2023_active.graphml\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "710c0e3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Users bridging multiple communities: 4905\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Apply Louvain\n",
    "import community as community_louvain\n",
    "partition = community_louvain.best_partition(G)\n",
    "nx.set_node_attributes(G, partition, 'community')\n",
    "\n",
    "# Step 2: Detect overlap\n",
    "overlap_users = set()\n",
    "\n",
    "for u, v in G.edges():\n",
    "    comm_u = G.nodes[u]['community']\n",
    "    comm_v = G.nodes[v]['community']\n",
    "    if comm_u != comm_v:\n",
    "        overlap_users.add(u)\n",
    "        overlap_users.add(v)\n",
    "\n",
    "print(f\"Users bridging multiple communities: {len(overlap_users)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d958c0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mark all bridge users with an attribute\n",
    "for node in G.nodes():\n",
    "    G.nodes[node]['is_bridge'] = int(node in overlap_users)\n",
    "\n",
    "# Export updated graph\n",
    "nx.write_graphml(G, \"bridge_users_graph.graphml\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3448998f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”— Total Nodes: 5188\n",
      "ðŸ”— Total Edges: 550239\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import networkx as nx\n",
    "import community as community_louvain  # Louvain module\n",
    "import os\n",
    "\n",
    "# --- Load cleaned Reddit data ---\n",
    "def load_reddit_json(path):\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "# Update to match your actual filenames\n",
    "data1 = load_reddit_json('metgala_data_cleaned/metgala_popculturechat_2023.json')\n",
    "data2 = load_reddit_json('metgala_data_cleaned/metgala_Fauxmoi_2023.json')\n",
    "\n",
    "# Combine data\n",
    "combined_data = data1 + data2\n",
    "\n",
    "# --- Build user interaction graph ---\n",
    "G = nx.Graph()\n",
    "\n",
    "for post in combined_data:\n",
    "    post_author = post.get(\"author\", \"N/A\")\n",
    "    commenters = [comment[\"author\"] for comment in post.get(\"comments\", []) if comment[\"author\"] != \"N/A\"]\n",
    "\n",
    "    # Add edges between post author and commenters\n",
    "    for commenter in commenters:\n",
    "        if post_author != \"N/A\":\n",
    "            G.add_edge(post_author, commenter)\n",
    "\n",
    "    # Optionally connect commenters with each other\n",
    "    for i in range(len(commenters)):\n",
    "        for j in range(i + 1, len(commenters)):\n",
    "            G.add_edge(commenters[i], commenters[j])\n",
    "\n",
    "# --- Louvain Community Detection ---\n",
    "print(f\"ðŸ”— Total Nodes: {G.number_of_nodes()}\")\n",
    "print(f\"ðŸ”— Total Edges: {G.number_of_edges()}\")\n",
    "\n",
    "partition = community_louvain.best_partition(G)\n",
    "nx.set_node_attributes(G, partition, 'community')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a83c9ff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Final Nodes: 5188\n",
      "Final Edges: 550239\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Keep largest connected component (removes isolated groups)\n",
    "G_largest = max(nx.connected_components(G_filtered), key=len)\n",
    "G_final = G_filtered.subgraph(G_largest).copy()\n",
    "\n",
    "print(f\"Final Nodes: {len(G_final.nodes())}\")\n",
    "print(f\"Final Edges: {len(G_final.edges())}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eae203cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§‘â€ðŸ¤â€ðŸ§‘ Nodes (Celebrities): 115\n",
      "ðŸ”— Edges (Co-mentions): 333\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "import networkx as nx\n",
    "from itertools import combinations\n",
    "\n",
    "# --- Step 1: Load attendee names ---\n",
    "celeb_df = pd.read_csv(\"attendees.csv\")\n",
    "celeb_names = celeb_df['Name'].dropna().str.lower().unique()\n",
    "celeb_patterns = [re.escape(name) for name in celeb_names]\n",
    "celeb_regex = re.compile(r'\\b(?:' + '|'.join(celeb_patterns) + r')\\b')\n",
    "\n",
    "# --- Step 2: Load Reddit data ---\n",
    "with open(\"metgala_data_cleaned/metgala_popculturechat_2025.json\", \"r\", encoding=\"utf-8\") as f1:\n",
    "    data1 = json.load(f1)\n",
    "with open(\"metgala_data_cleaned/metgala_Fauxmoi_2025.json\", \"r\", encoding=\"utf-8\") as f2:\n",
    "    data2 = json.load(f2)\n",
    "\n",
    "combined_data = data1 + data2\n",
    "\n",
    "# --- Step 3: Initialize Graph ---\n",
    "G = nx.Graph()\n",
    "\n",
    "# --- Step 4: Scan for co-mentions ---\n",
    "def extract_mentions(text):\n",
    "    return list(set(celeb_regex.findall(text.lower())))\n",
    "\n",
    "for post in combined_data:\n",
    "    texts = [post.get(\"title\", \"\") + \" \" + post.get(\"selftext\", \"\")]\n",
    "    texts += [comment.get(\"body\", \"\") for comment in post.get(\"comments\", [])]\n",
    "\n",
    "    for text in texts:\n",
    "        mentioned = extract_mentions(text)\n",
    "        if len(mentioned) > 1:\n",
    "            for celeb1, celeb2 in combinations(sorted(mentioned), 2):\n",
    "                if G.has_edge(celeb1, celeb2):\n",
    "                    G[celeb1][celeb2]['weight'] += 1\n",
    "                else:\n",
    "                    G.add_edge(celeb1, celeb2, weight=1)\n",
    "\n",
    "# --- Step 5: Graph Summary ---\n",
    "print(f\" Nodes (Celebrities): {G.number_of_nodes()}\")\n",
    "print(f\" Edges (Co-mentions): {G.number_of_edges()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "464316e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“¤ Exported as 'celeb_co_mentions.graphml'\n"
     ]
    }
   ],
   "source": [
    "# --- Step 6: Export to Gephi ---\n",
    "nx.write_graphml(G, \"celeb_co_mentions.graphml\")\n",
    "print(\"ðŸ“¤ Exported as 'celeb_co_mentions.graphml'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e72a5680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Nodes: 4553, Edges: 370085\n",
      "Graph exported to 'fauxmoi_2025_community.graphml'\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import networkx as nx\n",
    "from community import community_louvain\n",
    "\n",
    "# ---------- CONFIG ----------\n",
    "file_path = \"metgala_data_cleaned/metgala_Fauxmoi_2025.json\"\n",
    "output_path = \"fauxmoi_2025_community.graphml\"\n",
    "# ----------------------------\n",
    "\n",
    "# Step 1: Load data\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    posts = json.load(f)\n",
    "\n",
    "# Step 2: Build interaction graph\n",
    "G = nx.Graph()\n",
    "for post in posts:\n",
    "    author = post.get(\"author\", \"N/A\")\n",
    "    commenters = [c[\"author\"] for c in post.get(\"comments\", []) if c.get(\"author\") != \"N/A\"]\n",
    "\n",
    "    for commenter in commenters:\n",
    "        if author != \"N/A\":\n",
    "            G.add_edge(author, commenter)\n",
    "\n",
    "    for i in range(len(commenters)):\n",
    "        for j in range(i + 1, len(commenters)):\n",
    "            G.add_edge(commenters[i], commenters[j])\n",
    "\n",
    "print(f\" Nodes: {G.number_of_nodes()}, Edges: {G.number_of_edges()}\")\n",
    "\n",
    "# Step 3: Community detection\n",
    "if G.number_of_nodes() > 1:\n",
    "    partition = community_louvain.best_partition(G)\n",
    "    nx.set_node_attributes(G, partition, 'community')\n",
    "    \n",
    "    # Step 4: Export to Gephi-readable file\n",
    "    nx.write_graphml(G, output_path)\n",
    "    print(f\"Graph exported to '{output_path}'\")\n",
    "else:\n",
    "    print(\" Graph too small to detect communities.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b4be95",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
